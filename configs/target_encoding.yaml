name: target_encoding

featurize:
  name: base
  args: {}
#         pretrained_model_name_or_path: cointegrated/rubert-tiny
#         max_seq_length: 500
#         batch_size: 500

pipeline:
  name: target_encoding
  features: 
    - salary_rating
    - team_rating
    - managment_rating
    - career_rating
    - workplace_rating
    - rest_recovery_rating
    - country
    - type
    - population
    - latitude
    - longitude
    - position_lang
    - position_tokens_count
    - position_have_company
    - position_empty
    - position_unique_tokens_count
    - position_no_alpha_count
    - positive_lang
    - positive_tokens_count
    - positive_have_company
    - positive_empty
    - positive_unique_tokens_count
    - positive_no_alpha_count
    - negative_lang
    - negative_tokens_count
    - negative_have_company
    - negative_empty
    - negative_unique_tokens_count
    - negative_no_alpha_count
    - max_rating
    - min_rating
    - avg_rating
    - med_rating
    - 1_rating_count
    - 1_rating_frac
    - 2_rating_count
    - 2_rating_frac
    - 3_rating_count
    - 3_rating_frac
    - 4_rating_count
    - 4_rating_frac
    - 5_rating_count
    - 5_rating_frac
    - entropy_rating
    - positive_tokens_frac
    - positive_unique_tokens_frac
    - negative_unique_tokens_frac
    - positive_no_alpha_frac
    - negative_no_alpha_frac
    - positive_negative
    - have_unknown_symbol
  target:
    transformer: 
      class: competition.estimators.MapTransformer
      args:
        map: {0: 0, 1: 1, 2: 3, 3: 6, 4: 7, 5: 8, 
              6: [1, 6], 7: [1, 8], 8: [3, 8], 9: [5, 8], 10: [6, 8], }
        n_labels: 9
  transformers:
  -
    name: encoder
    columns: 
    - country
    - type
    - position_lang
    - positive_lang
    - negative_lang
    estimators:
      -
        name: ordinal
        class: sklearn.preprocessing.OrdinalEncoder
        args:
          handle_unknown: use_encoded_value
          unknown_value: .nan 

  selectors: 
  -
    name: dummy 
    class: competition.estimators.DummySelector
    args: {}
  model:
    class: lightgbm.LGBMClassifier
    args: 
      boosting_type: gbdt
      objective: multiclass
      seed: 110894
  fit_params: {}

  searcher:
    class: sklearn.model_selection.GridSearchCV
    args: 
      param_grid: 
        model__n_estimators: [25, 50, 100, ] 
        model__num_leaves: [20, 30, 40, 50, 60] 
        model__min_child_samples: [20, 40, 60, 80, 100] 
        # model__bagging_freq: [1, ] 
        # model__bagging_fraction: [0.75, 0.8, 0.85, 0.9, 0.95] 
        model__feature_fraction: [0.75, 0.8, 0.85, 0.9, 0.95]
        # model__learning_rate: [0.1, 0.01, 0.001]
      scoring: f1_macro
      cv:
        class: sklearn.model_selection.ShuffleSplit
        args:
          n_splits: 2
          test_size: 0.3
          random_state: 110894
      verbose: 4
  
  evaluation:
    split:
      class: sklearn.model_selection.ShuffleSplit
      args:
        n_splits: 2
        test_size: 0.3
        random_state: 110894
    metrics:
      -
        name: f1
        type: sklearn.metrics.f1_score
        args:
          average: samples 
        callback: false
